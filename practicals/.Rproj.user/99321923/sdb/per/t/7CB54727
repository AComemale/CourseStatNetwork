{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Paper AOAS1703-027, round 1: answers to the questions raised by the referees'\nauthor: \"J. Chiquet, M. Mariadassou and S. Robin\"\ndate: \"13 septembre 2017\"\noutput: \n  pdf_document:\n    number_sections: true\n    includes:\n      in_header: ../20170913_revision/Commands.tex\ncsl: apa-no-doi-no-issue.csl\nbibliography: ../20170913_revision/CMR17.bib\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\nDear Editor and Associate Editor,\n\nWe are grateful to the Editor, to the Associate Editor and to the referee for their time and efforts in providing very constructive and helpful comments that have led to clarify and substantially improve the quality of the paper.  We have made our best to change the manuscript by following the referee's suggestions. Our manuscript has been fully revised. You will find hereafter our answers to the questions and comments raised by the referees. The changes we made in the paper are in red in the manuscript.\n\n# Answer to the questions raised by reviewer 'E1'\n\n## Why variational method\n\n_The authors presented the methodology for the variational approximation to the exponential family PCA models. Although their method is actually applied to the real data analysis of two bacteria datasets in the paper, it is not explained in detail why the variational method is adequate and useful for these datasets. Since the adequacy and usefulness of the proposed methodology to the applied analyses is the key for the journals of applied studies such as AoAS, I would like the authors to clarify this point in the paper. To be more specific, I rephrase my point in the two specific questions in the following._\n\nOur aim is to provide practioners with a useful, flexible and computationally efficient statistical framework to analyse large count tables, such as those encountered in microbial ecology.\nWe rewrote the introduction to emphasize the introduction of covariates and offsets in the proposed modelling, which is one major novelty of our approach. We explain better why standard inference approaches (such as EM or Gibbs sampling) are not feasible for non-Gaussian pPCA. We also emphasize that the choice of variational approximation for pPCA leads to an efficient inference algorithm. Finally, we demonstrate that our method can truly deal with high-dimensional ($p \\simeq 4000$) data.\n\n### Comparison with the other options, especially MCMC and simulation-based methods\n\n_In the paragraph staring at the end of page 6, the authors write the reason that the variational approach is considered in this paper as “Numerical approximation using Hermite-Gauss quadrature or MCMC techniques are possible but likely to become computationally prohibitive as the dimension of the integration space increases.” This statement is correct in general, but not necessarily true in the applied problems considered in this paper. The first example, where n = 116 and p = 114, can be analyzed by the standard MCMC method (the algorithm im- plied by the Gibbs sampler and independent Metropolis-Hastings), although it may take some hours or a few days. The second example is originally high-dimensional with p = 4031 but later some of the data are discarded so that p = 500, which is moderately large data but still the simulation-based analysis can be applicable. Unlike the on-line analysis of processing the steaming data in a timely manner, the main objective in these two problems is the deep understanding of the microbiological structure, so the computational time is not prioritized to the accuracy and efficiency of the statistical analysis. The authors should explain  why the other approaches are not suitable for the examples used in this paper._\n\n\nIn the introduction, we now explain that no conditional distribution is known, so that regular Gibbs sampling can not be used. For this reason, @LeS01 use moment estimation whereas @LiT10 resort to variational approximation even for Bayesian inference (see Introduction and Section 2.4).\n\nOur purpose is to be able to handle datasets with several thousands of variables, and we demonstrate that the proposed algorithm does so on the piglet example of the Illustration section. We also use the piglet example to discuss the increase in noise induced by keeping non-informative species in the analysis. In practice we run the algorithm on the whole dataset (if only to assess running times) but only present the results obtained on a smaller, more informative dataset. \n\n### Accuracy/validity of the approximation\n\n_If the estimation of the original model is available (by either Frequentist or Bayesian methods), it is a great gain that we can quantify the uncertainty of estimation, while the variational method only gives the point estimates of the parameters of interest. For example, the predictive analysis in the paragraph named “Variance of the variational posterior” in page 17 is surely outperformed by that of the Bayesian predictive distribution of Z ij or Y ij . My question here is whether the variational approach can provide the similar performance to that of the full analysis of the original model. The authors can conduct an additional analysis, for example, to compare the posterior and predictive performance with that of the full model to see if the result obtained by the variational method is not drastically different from the analysis without variational approximation._\n\nWe acknowledge your remark, which is due to an unfortunate naming mistake. The paragraph should have been mentioned \"conditional distribution\" as we don't perform inference in a Bayesian setting. We have corrected that. \n\nRegarding estimation of the original model, it is unavailable using frequentist methods as soon as $q$ is moderate ($\\geq 10$) as it requires multidimensional integrals of the form $\\mathbb{E}[\\exp(-\\exp(u^\\intercal b))]$ where $u$ is a $q$-dimensional multivariate Gaussian. Even in a Bayesian setting, @LiT10 resort to a variational approximation instead of computing the posterior as it is computationnally too difficult. We therefore have no mean to compare the variational approximation to the full analysis. Another major difference that we stress more in the current revision between our approach and others is the inclusion of covariates in the model.\n  \n## How good is the variational method\n\n_I am afraid that the paper still lacks the explanation on the model and methodology. I have three questions on the model, methodology and their advantage._\n\n### Loading matrix B\n\n_The identification of the loading matrix B is crucial in PCA. There is no explanation on what type of identification constraint is placed in the proposed methodology. The analyses shown in Fig. 1c and Fig. 5 would be greatly affected if one changes the constraints on the loading. I would like the authors to comment more on what type of B is expected to be obtained by the proposed Newton-type algorithm and how the resulted point estimates, such as $\\hat{B}$ and $\\hat{B}$, should be interpreted._\n\nWe agree with the statement and we reinforced Remark $1$ to clarify this. The identifiable parameters in our model are $\\Sigma = BB^\\intercal$ and $\\mu$. Even standard PCA amounts to approximating data $Y$ by its projection in a $q$-dimensional affine space (the product $MB^\\trans$ in our case). It can be framed as the following (not very clever) optimization problem:\n$$\n\\min_{M \\in \\mathbb{R}^{n \\times q}, B \\in \\mathbb{R}^{p \\times q}, \\mu \\in \\mathbb{R}^{1 \\times p}} \\| Y - (1_n \\mu + MB^\\intercal) \\|_F^2\n$$\nwhere the product $MB^\\intercal$ can be obtained by infinitely many couples $(M, B)$. Imposing orthogonality constraints on $B$ and principal components ($M$) of decreasing variance is necessary to uniquely recover $B$ and $M$. Orthogonality constraints are also very useful in the Gaussian setting to decompose a $q$-dimensional approximation problem into a series of $q$ (easier) one-dimensional problems. \n\nThe Poisson likelihood induces a non-linear projection from the observation to the parameter space. It means that we don't inherit the decomposition property and have to address the estimation of $B$ head-on rather than axis by axis. The corresponding optimization problem is made easier by not imposing constraints on $B$ and we recover latent positions $MB^\\trans$. When visualizing those positions and to keep in spirit with PCA, we express them in an orthogonal basis by performing a PCA $MB^\\trans$. This does however not affect the estimate $\\hat{\\Sigma}$. \n\nWe rewrote Remark $1$ and Section $6$ to clarify these points.\n\n### Small count data\n\n_Another issue is about the accuracy of the variational approximation. In the second application, more than 2000 data are discarded since the observed values are too small. Naturally, the remaining observations must take the sufficiently large values, where the central limit theorem kicks in and the normal approximation of Poisson distribution for the large rate is guaranteed, justifying the use of the Gaussian PCA model in Equation (2) as a good approximation. The choice of Poisson distribution as the observational structure is important rather for the small values that are discarded in this study, and I wonder if the proposed variational approximation is still reliable with these small counts. From this viewpoint, the variational analysis for the larger dataset in “Numerical Experiments” paragraph in page 17-18 should be developed further in the paper._\n\nWe agree that the Poisson distribution is not appropriate to model over-dispersion. But Poisson log-normal -- or compound distribution with Poisson and randomness on the Poisson parameter -- is largely over-dispersed (for example, Poisson-Gamma gives rise to the over-dispersed Negative-Binomial). Furthermore, a latent variable with a large negative results in a large probability mass in $0$. We insisted more on this point in the Introduction and in Section $5$. \n  \nWe also agree that the conditional distribution has a large variance when observed counts are smalls, but it is due to a lack of information rather than to the variational approximation. The following graph shows the conditional distributions of $Z|Y=y$ for various values of $y$ in the unidimensional Poisson-Normal model where $Z \\sim \\mathcal{N}(0, 1)$ and $Y|Z \\sim \\mathcal{P}(\\exp(Z))$. It clearly shows that the conditional has a much higher variance for low values of $y$. \n\n```{r}\nn <- 1e4\nZ <- rnorm(n)\nY <- rpois(n, exp(Z))\nplot(Y, Z, pch=20, xlab = \"Y\", ylab = \"Z\")\n```\n\nRegarding the more than 2500 OTUs that were discarded in the second example, they are very sparse: $99$% of the entries in that sub-table are $0$ and the remaining counts are very small (90% of 1, 9% of 2 and 1% of 3). They also correspond mostly (50%) to \"singletons\" *i.e.* OTUs found only once in only one sample and are typically discarded in microbiome analysis as they correspond essentially to high dimensional noise (chimeric sequences, sequencing errors, etc). We also emphasize that even the small filtered count table with only the 500 most abundant species is very sparse (60%) and has a lot of small counts (60% of non null entries are smaller than or equal to 4), for which the Gaussian approximation to the Poisson is unlikely to kick in. We detail this point in Section 7.1.\n\n### The gain and new finding from the new variational PCA\n\n_Several comments stated above can be rephrased as the question on the contribution of this research to the applied problem. The existing paper, Mach et al. (2015), has already found the principle components that explain the effect of weaning for bacteria in piglets. Hence, I am afraid that the results presented in this paper might be regarded as just re-confirming the statistical fact that is already known. It would be necessary for this paper to show, or explain, the advantage of the variational approach to this particular applied problem._\n\nIn Section 7.1, we use the piglet example to check the ability of our method to recover a known ground truth and to assess scalability. Regarding this point, new developments in the implementation were made, which considerably decrease the computational burden (Figure 1). We moved to a different gradient-based procedure to maximize the variational lower bound, as mentioned in Sections 4.1 and 5.3.\n \nIn Section 7.2, we provide a more detailed analysis of the oak example. In absence of covariates, microbiologists would only get Fig.5 (bottom left), which only allows to discriminate between three trees (i.e. an obvious first-order effect). Including the covariates gives access to second-order effects. Once accounted for the tree effect, pPCA shows that the samples are organised according to the distance from the leaf to the ground. The residual correlation between species abundances is also drastically different from the one before correction. This is now shown in Fig.$5$ (bottom).\n\n## Additional points\n\n1. [page 1]\n\n_The last sentence of the first paragraph reads \"From a purely algebraic... PCA can be seen as a matrix-factorization problem were the data matrix is ... ,\" but I think this is \"where.\"_\n\nDone\n\n2. [page 4]\n\n_Following Equation (1), it reads \"Furthermore, the latent vectors are conventionally assumed to have independent Gaussian component with unit variance $\\sigma^2 = 1$, that is to say,  $\\varepsilon_i\\sim\\mathcal{N}(0_q, I_q)$\". This should be about the factor $w_i$, not the observational error $\\varepsilon_i$, so I guess $w_i\\sim\\mathcal{N}(0_q, I_q)$ should be put here. Also, $\\sigma^2=1$ is confusing and should be removed because $\\sigma^2$ is the observational error._\n\nThank you for pointing out this error, which we corrected.\n\n3. [page 13, Section 6.1]\n\n_There are some notation that is not explained here. I guess $(\\hat{\\theta},\\hat{B})$ should be the point estimate computed by the proposed Newton-type method, but why $\\tilde{m}$ is used instead of $\\hat{m}$? What implication does this difference of notation have?_\n\nIn the first case, $(\\hat{\\theta},\\hat{B})$ is indeed the point estimate of the model parameters, that is, the parameter of interest and we use the standard notation $\\, \\hat{}$. In the second case,  $(\\tilde{M},\\tilde{S})$ are the variational parameters that minimize the $KL$ divergence and are therefore auxiliary terms used in the inference. We think it is important to make that distinction. However, it was never said in the paper so we added a comment on this in Section 6.\n\n4. [page 13, Section 6.2]\n\n_The equation in Section 6.2 is_\n\\begin{equation*}\n  \\ell_q = \\sum_{i,j}  [Y_{ij} g(\\lambda_{ij}^{(q)}) - Y_{ij}] - K(Y).\n\\end{equation*}\n_where the second $Y_{ij}$ should be $b(\\tilde{Z}_{ij})$. I think the authors can focus on the Poisson model here, avoiding the notation $g(\\lambda)$ that is not used anywhere else._\n\nThank you for pointing out the mistake. We corrected it and only give formula for the Poisson model.\n\n5. [page 16, \"Effect of the covariates\"]\n\n_The parameter $\\Sigma$ is not defined explicitly. It should be about the covariance of abundance measures, so I guess it is the variance of Poisson rate $\\Sigma = Var[Z]$, or factors $\\Sigma = B B'$ or $\\Sigma = B B'+ I_q$. There $(q)$ is a similar notation $\\Sigma$ GLM in page 12, but $\\Sigma$ in page 16 is not exactly the same model._\n\nIndeed, the use of the notation $\\Sigma$ was confusing. We explicitly add $\\Sigma$ in the definition of our model in Equations (2) and (4) and added comments along this section.\n\n# Answer to the questions raised by reviewer 'R1'\n\n## Motivation of Authors' Approach:\n\n_Generalizing pPCA to various natural exponential family distributions seems to be a fairly well studied problem. With this in mind, the authors must distinguish why their proposed approach is an important contribution. As the authors discuss on page 3, it appears that the primary contribution of this paper is that they consider the scores as a parameter rather than a random variable. While they claim that \"this has deep consequences for the general properties of the inference algorithm\" this comment never explained and seems like a severely undeveloped motivation for their approach._ \n\nIndeed, our contribution was not clear enough so we rewrote the introduction, emphasizing the introduction of covariates (and offset) and the interest of the variational approximation to get a scalable algorithm (see response 2.1.1 to the AE). Setting the scores as random and the loadings as fixed sets us in the exact framework of pPCA, as defined by @TiB99. We agree that the claim was unclear and removed it. \n\n### Clarity of Writing:\n\n_Beyond the motivation of their approach, there are a number of other places where the writing is unclear. While there are frequent examples of this throughout the paper, I mention just a few below._  \n\n1. _Frequently, the authors notation is excessively confusing (e.g., reuse of $\\varepsilon_i$ for both the observation error and error of the loadings on page 4; the definition of $\\Sigma$ is never provided on page 16)._\n\nThank you for pointing these errors, that we corrected. We added a proper definition of $\\Sigma$ in the Model definitions in Section 2 and removed the ambiguity with $\\varepsilon$. \n\n2. _Why is the BIC and ICL calculations in equation (14) presented as \"pseudo-BIC\" and \"pseudo-ICL\" this should be explained._\n\nThe true BIC is admittedly computed with the true likelihood. In our model, we only have access to a variational lower bound of that quantity, which explains the \"pseudo\"-BIC. However, we agree that the phrasing was unnecessarily confusing. We rewrote the paragraph 4.5 to make that clear.\n\n3. _It seems as though the missing data handling suggested by the authors relates to data missing at random, this should be better explained._\n\nIndeed, when data are not missing at random, our algorithm induces bias in the estimation of the parameters if the process originating the missing data is not taken into account in the inference. This is the case for most EM approaches, that impute missing data implicitely under MAR conditions. We clarify this point in the paper and now only claim that we can handle missing data and that our imputation is correct in the MAR case.\n\n4. _On page 13 the authors state that $\\tilde{Z}$ is most useful to assess goodness of fit. Are the authors alluding to the later description in section 6.2 where $\\lambda$ is defined in-terms of $\\tilde{Z}$?_\n\nThis is indeed the case: we define $\\lambda_{ij} = \\tilde{Z}_{ij}$ (and its counterpart in the _null_ and _saturated_ models). \n\n5. _Why is the authors description of $R^2_q$  useful? It seems as though this is a poor measure that later in the manuscript needs to be corrected to be of use. Furthermore, the authors final statement in section 6.2 related to the nondecreasing nature of $R^2_q$ is not properly explained; why would $R^2_q$ be nondecreasing if $\\ell_q$ was the objective function of the variational inference? What are the consequences of having $R^2_q$ not be nondecreasing? Beyond this, it seems as though the measure $R^2_q$ requires strong modeling assumptions as it ultimately relies on a \"saturated model\" and a Poisson GLM null model; misspecification of either would seem to be a potential problem for this metric._\n\nWhen performing PCA, the percentage of explained variance and its decomposition along principal components are natural outputs and practitioners have grown to expect them. We thought it was important to produce a similar, although imperfect, criterion for our model. We also agree that defining the _null_ and _saturated_ model in terms of Poisson model deprives us of over-dispersed counts but we think that the benefits of proposing counterparts to familiar quantities overrides the caveats of our pseudo-R$^2$. We shortened this part and clarified the computation of our $R^2$ in Section 6.2. \n\n## Demonstration/Validation of Method:\n\n_The computational correctness and utility of the authors models appears to hinge on their illustrations on microbiome data in section 7; however, there are a number of questionable modeling choices in this section that I believe undermine these illustrations._ \n\nPCA is already widely used for analyzing this kind of data. The major contribution of our work is to provide a variant of this model that can account for covariates and for which we derive a new efficient inference algorithm. The illustrations we propose show how the introduction of covariates  helps to better understand the dependency structure between the species abundances. Typically, not accounting for first-order effects can dramatically change the observed correlations, and thus our understanding of the interactions between the species.\n\n1. _The authors do not actually describe what their dataset is. They claim that their dataset \"consists in abundances measures of 55 bacterial species and 48 fungal species\", however it is unclear what an abundance measure is or how such a measure is obtained. In particular I would emphasize that metabarcoding is a non-specific description. Furthermore, bacterial species are never measured directly, are the authors referring to OTUs or perhaps OTUs that have been assigned taxonomic identifies and then amalgamated at the species level? Is every bacteria on every oak leaf counted? As this is almost certainly not the case, the dataset likely represents multivariate counts that only reflect the relative abundances of the bacteria/fungi in their original environment. Such a situation is then inappropriately modeled using univariate counts distributions. The authors discussion of differing total abundances highlights that this is almost certainly the case and their use of an offset term $o_{ij}$ to deal with this situation seems bizarre and inappropriate._ \n\nYes, we  refer to OTUs and agree that the description of the dataset was too laconic. We modified the text in Section 7 to refer to OTUs instead of species. We also give a short description about the marker genes and procedures used for OTU picking but still refer to @Mach2015 and @JFS16 for full details. \n\nWe also agree that amplicon metagenomics only gives access to multivariate counts that represent relative abundances but disagree with the next remarks: our model create fully multivariate counts distributions. The observation noise $Y_{ij}|Z_{ij}$ are univariate but the latent variable $Z$ is not and the coordinates of $Y_i$ are correlated through their dependence on $Z$. We insist on this point in Section 5.1. The fact that total abundances differ by samples is a consequence of the randomness of sequencing technologies and we account for it using a sample-wise offset ($o_{i1} = \\dots = o_{ip} = o_{i}$). The offset captures the fact that _et ceteris paribus_, abundances should be roughly twice as high in a sample sequenced twice as deeply. We don't see how that is bizarre nor inappropriate. \n\nIn general, we expect the offset to be the same for all OTUs in a sample but we write $o_{ij}$ to cover more general situations. This turns out to be important in the @JFS16 dataset where abundances for bacterial and fungal OTUs are measured on the same samples but using a different metabarcoding marker for each type of microbe. We thus use two offsets per sample in that dataset: one for bacterial OTUs and one for fungal ones. \n\n2. _The study of microbiomes is often complicated by a lack of ground truths, as such it seems entirely possible that any model applied to these dataset would provide a result that could be claimed was important or correct. It seems the authors would be much better served by including analysis of a separate dataset with known ground truths and/or inclusion of simulated data to at least demonstrate the computational correctness of the algorithm._\n\nThis is a very fair remark. However, the structuring effect of weaning (and more generally of diet change) is very strong and exploratory methods with very different rationales recover it consistently. We therefore consider it as ground truth in our first example and show that we recover it. \n\nConsidering the estimation of $\\Sigma$, it is very difficult to find examples where both the dependency structure between the OTUs/species is known and the problem size requires dimension reduction. We are also reluctant to add a full-fledged simulation study, which would make the paper longer. When the data are simulated according to the model, the methods performs well and recovers the parameters quite accurately (see the brief study at the end of this document) but this is of limited practical relevance, as real abundances are always misspecified. We prefer to use our method on real datasets and show that it recovers the known structure, such as weaning in the piglet dataset, or unravels new structure, such as the altitude gradient in the oaks example, once first order effects are removed by including them as covariates in the model. \n\n3. _The description on page 18 of how removing the least abundant species changes the dimension of the latent subspace seems out of place and inconsequential. Why is this important or unexpected?_\n\nWe now describe a bit more the features of the least abundant OTUs. The OTU subsets are mostly used to assess scalability and confirm that computation time increases linearly with $p$. Neverthelesse, since we actually estimated the model we deemed it noteworthy to comment briefly on the results, even though they were expected: the least abundant OTUs behave as high dimensional noise, increase computation time and degrade the ability of the method to find fine structure. \n\n4. _Given that the authors' method relies on a variational approximation, it seems crucial that the authors demonstrate that this approximation provides useful results and also discusses when this approximation breaks down, this has not been done._\n\nThis is another fair remark. We now reinforce the fact that variational approximation makes the model tractable. As stated in the introduction and to the best of our knowledge, variational approximation (our work and also used in @LiT10 in a Bayesian setting) and moment estimation (@LeS01) are the only methods available for parameter estimation in this model and perform well in practice (see point 2 raised by the AE). \n\nWe also reworked our second example (Section 7.2) to drive home the usefulness of our methods. We show that accounting for covariates, one of the salient features of our model, allows microbial ecologists to explore and discover second-order effects (here altitude to the ground) that are masked by strong first order effect (here sampling tree).  \n\n5. _It appears that interpretation of model outputs is very much non-trivial and the authors rely on standard principle component analysis to interpret the results of their poisson probabilistic principle component analysis. All this seems to stem from a lack of orthogonality constraints in their model. Why are such orthogonality constraints not used or why does this no make sense? Even if this is a standard issue with non-Gaussian probabilistic PCA such a question must be addressed._ \n\nLack of orthogonality is indeed a standard issue with non-Gaussian probabilistic PCA and was poorly explained, a concern also raised by the AE. We therefore rewrote Section 6 and remark 1 to emphasize that point and identify potential problems with $B$. Briefly (but see answer to point 1.2.1), we leave $B$ unconstrained during optimization but orthogonalize it for visualization purpose. This leaves the estimate $\\hat{\\Sigma}$ and $\\hat{\\Theta}$ unchanged but provides visualization of the samples in the parameter space that are consistent with Gaussian PCA. \n\n## Simulation study \n\nWe add here a short simulation study to illustrate the good performances of our inference algorithm. We simulated 100 data sets with same dimensions as the oak mildew dataset: $n = 116$, $p=114$, $d=3$. We considered $q=10$ and $q=20$ latent dimensions. For each dimension $q$, the results are summarized in 6 plots organized as follows: \n$$\n\\begin{tabular}{|p{.4\\textwidth}|p{.4\\textwidth}|}\n   \\hline\n   Example of the counts distribution ($\\log_{10}$) & Example of infered cooordinates of the latent  position vs true ones \\\\\n   \\hline\n   Mean (over 100 replicates) estimates of $\\theta_{kj}$ vs true values & Mean (over 100 replicates) estimates of $\\sigma_{k\\ell}$ vs true value \\\\\n   \\hline\n   Boxplot (for 100 replicates) of the canonical correlation between the true latent space and the estimated one & Boxplot (for 100 replicates) of the correlation between the true latent coordinates and the estimated ones \\\\\n   \\hline\n\\end{tabular}\n$$\nThe simulations display count distribution that are simular to these of our illustrations (top left). The results show that the estimates of both $\\Theta$ (middle left) and $\\Sigma$ (middle right) are quite accurate (as well as the recontruction of the latent hyperplane: bottom left) and the latent positions are also well reconstructed (bottom right), although the suffer a higher variability (top right).\n\n### Results for $q = 10$ \n```{r,echo=F}\nn = 116; p = 114; d = 3; o = 0; sim.nb = 100; q = 10; \n# Functions\nF_Vec2Sym <- function(A.vec, diag=F){\n   # Makes a symmetric matrix from the vector made of its lower tirangular part\n   if(diag){n = (-1+sqrt(1+8*length(A.vec)))/2}else{n = (1+sqrt(1+8*length(A.vec)))/2}\n   A.mat = matrix(0, n, n); A.mat[upper.tri(A.mat, diag=diag)] = A.vec\n   A.mat = A.mat + t(A.mat); if(diag){diag(A.mat) = .5*diag(A.mat)}\n   return(A.mat)\n}\n# Parms\npar(mfrow=c(3, 2), pch=20, mex=.6)\n# Res\nload(paste('../../../Pgm/PCA/PCAsimulstudy-n', n, '-p', p, '-q', q, '.Rdata', sep=''))\nSigma.vec = as.vector(Res$Sigma[upper.tri(Res$Sigma, diag=T)])\nTheta.vec = as.vector(Res$Theta)\nSigma.eig.val = eigen(Res$Sigma)$values[1:q]\nSigma.eig.vec = eigen(Res$Sigma)$vectors[, 1:q]\n# Example\nhist(log10(1+Res$Y.last), breaks=sqrt(n*p), xlab='', ylab='counts (log10)', main='')\nplot(Res$Z.last, Res$Z.hat.last, xlab='true latent position', ylab='position estimate'); abline(a=0, b=1, col=2)\n# Theta \nTheta.mat = rep(1, sim.nb)%o%Theta.vec\nplot(Theta.vec, apply(Res$Theta.hat, 2, mean), xlab='true Theta', ylab='mean estimate'); abline(a=0, b=1, col=2)\n# Sigma\nplot(Sigma.vec, apply(Res$Sigma.bis, 2, mean), xlab='true Sigma', ylab='mean estimate'); abline(a=0, b=1, col=2)\n# Recompute all quantities with Sigma.bis (wrong package version!)\nSigma.bis.eig.val = matrix(0, sim.nb, p); Sigma.bis.CCA = rep(0, sim.nb)\ninvisible(sapply(1:sim.nb, function(s){\n   Sigma.bis.mat = F_Vec2Sym(Res$Sigma.bis[s, ], diag=T)\n   Sigma.bis.eig = eigen(Sigma.bis.mat)\n   Sigma.bis.eig.val[s, ] <<- Sigma.bis.eig$val\n   Sigma.bis.CCA[s] <<- sum(svd(crossprod(Sigma.bis.eig$vector[, 1:q], Sigma.eig.vec))$d)\n   }))\nboxplot(Sigma.bis.CCA/q, ylab='canonical correlation', ylim=c(.8, 1))\n# Z\nboxplot(Res$Z.cor, ylab='latent position correlation', ylim=c(.8, 1))\n```\n\n### Results for $q = 20$ \n```{r,echo=F}\nn = 116; p = 114; d = 3; o = 0; sim.nb = 100; q = 20; \n# Functions\nF_Vec2Sym <- function(A.vec, diag=F){\n   # Makes a symmetric matrix from the vector made of its lower tirangular part\n   if(diag){n = (-1+sqrt(1+8*length(A.vec)))/2}else{n = (1+sqrt(1+8*length(A.vec)))/2}\n   A.mat = matrix(0, n, n); A.mat[upper.tri(A.mat, diag=diag)] = A.vec\n   A.mat = A.mat + t(A.mat); if(diag){diag(A.mat) = .5*diag(A.mat)}\n   return(A.mat)\n}\n# Parms\npar(mfrow=c(3, 2), pch=20, mex=.6)\n# Res\nload(paste('../../../Pgm/PCA/PCAsimulstudy-n', n, '-p', p, '-q', q, '.Rdata', sep=''))\nSigma.vec = as.vector(Res$Sigma[upper.tri(Res$Sigma, diag=T)])\nTheta.vec = as.vector(Res$Theta)\nSigma.eig.val = eigen(Res$Sigma)$values[1:q]\nSigma.eig.vec = eigen(Res$Sigma)$vectors[, 1:q]\n# Example\nhist(log10(1+Res$Y.last), breaks=sqrt(n*p), xlab='', ylab='counts (log10)', main='')\nplot(Res$Z.last, Res$Z.hat.last, xlab='true latent position', ylab='position estimate'); abline(a=0, b=1, col=2)\n# Theta \nTheta.mat = rep(1, sim.nb)%o%Theta.vec\nplot(Theta.vec, apply(Res$Theta.hat, 2, mean), xlab='true Theta', ylab='mean estimate'); abline(a=0, b=1, col=2)\n# Sigma\nplot(Sigma.vec, apply(Res$Sigma.bis, 2, mean), xlab='true Sigma', ylab='mean estimate'); abline(a=0, b=1, col=2)\n# Recompute all quantities with Sigma.bis (wrong package version!)\nSigma.bis.eig.val = matrix(0, sim.nb, p); Sigma.bis.CCA = rep(0, sim.nb)\ninvisible(sapply(1:sim.nb, function(s){\n   Sigma.bis.mat = F_Vec2Sym(Res$Sigma.bis[s, ], diag=T)\n   Sigma.bis.eig = eigen(Sigma.bis.mat)\n   Sigma.bis.eig.val[s, ] <<- Sigma.bis.eig$val\n   Sigma.bis.CCA[s] <<- sum(svd(crossprod(Sigma.bis.eig$vector[, 1:q], Sigma.eig.vec))$d)\n   }))\nboxplot(Sigma.bis.CCA/q, ylab='canonical correlation', ylim=c(.8, 1))\n# Z\nboxplot(Res$Z.cor, ylab='latent position correlation', ylim=c(.8, 1))\n```\n\n\n## References  \n",
    "created" : 1507285617800.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "835807026",
    "id" : "7CB54727",
    "lastKnownWriteTime" : 1507285689,
    "last_content_update" : 1507285689227,
    "path" : "~/svn/sparsepca/Article/AOAS/comments_round1/answer_round1.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}